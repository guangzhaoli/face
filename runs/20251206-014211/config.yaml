dtype: bfloat16
flux_fill_path: /inspire/hdd/project/multimodal-discrete-diffusion/liuxiaohong-25080/guangzhaoli/face/model/flux_fill
flux_redux_path: /inspire/hdd/project/multimodal-discrete-diffusion/liuxiaohong-25080/guangzhaoli/face/model/flux_redux
model:
  add_cond_attn: false
  arcface_model: r100
  arcface_weights: /inspire/hdd/project/multimodal-discrete-diffusion/liuxiaohong-25080/guangzhaoli/face/model/arcface/backbone.pth
  face_loss_weight: 0.1
  latent_lora: false
  union_cond_attn: true
  use_face_loss: true
train:
  accumulate_grad_batches: 1
  batch_size: 16
  dataloader_workers: 8
  gradient_checkpointing: true
  lora_config:
    init_lora_weights: gaussian
    lora_alpha: 256
    r: 256
    target_modules: (.*x_embedder|.*(?<!single_)transformer_blocks\.[0-9]+\.norm1\.linear|.*(?<!single_)transformer_blocks\.[0-9]+\.attn\.to_k|.*(?<!single_)transformer_blocks\.[0-9]+\.attn\.to_q|.*(?<!single_)transformer_blocks\.[0-9]+\.attn\.to_v|.*(?<!single_)transformer_blocks\.[0-9]+\.attn\.to_out\.0|.*(?<!single_)transformer_blocks\.[0-9]+\.ff\.net\.2|.*single_transformer_blocks\.[0-9]+\.norm\.linear|.*single_transformer_blocks\.[0-9]+\.proj_mlp|.*single_transformer_blocks\.[0-9]+\.proj_out|.*single_transformer_blocks\.[0-9]+\.attn.to_k|.*single_transformer_blocks\.[0-9]+\.attn.to_q|.*single_transformer_blocks\.[0-9]+\.attn.to_v|.*single_transformer_blocks\.[0-9]+\.attn.to_out)
  max_steps: -1
  optimizer:
    params:
      lr: 1
      safeguard_warmup: true
      use_bias_correction: true
      weight_decay: 0.01
    type: Prodigy
  sample_interval: 500
  save_interval: 500
  save_path: runs


# person + head